## MCMC log-likelihood function and gradient using PyTensor for IAW
## Generated by: H. Poole

import numpy as np
from run_IAW import Run_OTS, Fit_OTS
import pytensor.tensor as pt
from pytensor.graph import Apply, Op
import pytensor
from scipy.optimize import approx_fprime

## Log-likelihood function for IAW without gradient

def loglike(te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data):
    """
    Computes the log-likelihood of observed data given a model for Thomson scattering analysis.

    This function generates a model spectrum using the provided plasma parameters and compares it to the observed data,
    returning the log-likelihood value. It performs input type checking, interpolates the model to the data points,
    applies scaling, and evaluates the likelihood.

    Parameters
    ----------
    te : float
        Electron temperature.
    ti : float
        Ion temperature.
    ne : float
        Electron density.
    e_cur : float
        Electron current or related parameter.
    flow : float
        Plasma flow velocity.
    v_grad : float
        Velocity gradient.
    sigma : float
        Scaling parameter for the model.
    core : float
        Core number model is being run on.
    x : np.ndarray
        Wavelength or position array for interpolation.
    data_err : np.ndarray
        Uncertainties associated with the observed data.
    data : np.ndarray
        Observed data to be fitted.

    Returns
    -------
    float
        The log-likelihood value comparing the model to the observed data.

    Raises
    ------
    TypeError
        If any input parameter is not a float or np.ndarray.
    """
    # Will fail explicitly if inputs are not numerical types for the sake of this tutorial
    # As defined, my_loglike would actually work fine with PyTensor variables!

    for param in (te, ti, ne, e_cur, flow, v_grad, sigma, x, data_err, data):
        if not isinstance(param, (float, np.ndarray)):
            raise TypeError(f"Invalid input type to loglike: {type(param)}")

    try: sigma = sigma[0]
    except: sigma = sigma

    model_x, model_y = Run_OTS().run_fitting(te, ti, ne, e_cur, flow, v_grad, core=core)
    model = np.interp(x, model_x*1e9, model_y)
    scaled_model = Fit_OTS().scalings(sigma, data, data_err, model)
    return Fit_OTS().likelihood(sigma, scaled_model, data, data_err)

class LogLike_nograd(Op):
    """
    A PyTensor Op for computing the log-likelihood of a model without gradient support.

    This class wraps a custom log-likelihood function (`loglike`) for use in PyTensor computational graphs,
    allowing it to be used in probabilistic programming or MCMC workflows where gradients are not required.
    The Op takes as input a set of model parameters and data arrays, and outputs the computed log-likelihood.

    Methods
    -------
    make_node(te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data)
        Constructs a PyTensor Apply node for the log-likelihood computation, converting all inputs to tensor variables.

    perform(node, inputs, outputs)
        Numerically evaluates the log-likelihood using the provided numpy arrays and stores the result in outputs.

    Parameters
    ----------
    te : array-like
        Electron temperature(s).
    ti : array-like
        Ion temperature(s).
    ne : array-like
        Electron density(ies).
    e_cur : array-like
        Current or energy parameter(s).
    flow : array-like
        Flow velocity(ies).
    v_grad : array-like
        Velocity gradient(s).
    sigma : array-like
        Broadening parameter(s).
    core : array-like
        Core parameter(s).
    x : array-like
        Independent variable(s) (e.g., wavelength or position).
    data_err : array-like
        Measurement uncertainties.
    data : array-like
        Observed data to fit.

    Returns
    -------
    loglike_eval : array-like
        The computed log-likelihood values, with the same shape and dtype as the input data.
    """
    def make_node(self, te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data) -> Apply:
        # Convert inputs to tensor variables
        te = pt.as_tensor(te)
        ti = pt.as_tensor(ti)
        ne = pt.as_tensor(ne)
        e_cur = pt.as_tensor(e_cur)
        flow = pt.as_tensor(flow)
        v_grad = pt.as_tensor(v_grad)
        sigma = pt.as_tensor(sigma)
        core = pt.as_tensor(core)
        x = pt.as_tensor(x)
        data_err = pt.as_tensor(data_err)
        data = pt.as_tensor(data)

        inputs = [te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data]
        # Define output type, in our case a vector of likelihoods
        # with the same dimensions and same data type as data
        # If data must always be a vector, we could have hard-coded
        # outputs = [pt.vector()]
        outputs = [data.type()]

        # Apply is an object that combines inputs, outputs and an Op (self)
        return Apply(self, inputs, outputs)

    def perform(self, node: Apply, inputs: list[np.ndarray], outputs: list[list[None]]) -> None:
        # This is the method that compute numerical output
        # given numerical inputs. Everything here is numpy arrays
        te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data = inputs  # this will contain my variables

        # call our numpy log-likelihood function
        loglike_eval = loglike(te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data)

        # Save the result in the outputs list provided by PyTensor
        # There is one list per output, each containing another list
        # pre-populated with a `None` where the result should be saved.
        outputs[0][0] = np.asarray(loglike_eval)

## Log-likelihood function for IAW with gradient
## This isn't really applicable for OTS but I leave it here for reference

def finite_differences_loglike(te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data, eps=1e-4):

    params = np.array([te, ti, ne, e_cur, flow, v_grad])

    def inner_func(params, sigma, core, x, data_err, data):
        te, ti, ne, e_cur, flow, v_grad = params
        calc = loglike(te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data).flatten()
        return calc

    epsil = np.zeros_like(params)+eps
    grad_wrt = approx_fprime(params, inner_func, epsil, sigma, core, x, data_err, data)

    return grad_wrt[:, 0], grad_wrt[:, 1], grad_wrt[:, 2], grad_wrt[:, 3], grad_wrt[:, 4], grad_wrt[:, 5]

class LogLikeWithGrad(Op):

    def __init__(self):
        self.additional_loglikegrad = LogLikeGrad()

    def make_node(self, te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data) -> Apply:
        # Convert inputs to tensor variables
        te = pt.sum(pt.as_tensor(te))
        ti = pt.sum(pt.as_tensor(ti))
        ne = pt.sum(pt.as_tensor(ne))
        e_cur = pt.sum(pt.as_tensor(e_cur))
        flow = pt.sum(pt.as_tensor(flow))
        v_grad = pt.sum(pt.as_tensor(v_grad))
        sigma = pt.as_tensor(sigma)
        core = pt.as_tensor(core)
        x = pt.as_tensor(x)
        data_err = pt.as_tensor(data_err)
        data = pt.as_tensor(data)

        inputs = [te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data]
        outputs = [data.type()]
        return Apply(self, inputs, outputs)

    def perform(self, node: Apply, inputs: list[np.ndarray], outputs: list[list[None]]) -> None:
        # Same as before
        te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data = inputs  # this will contain my variables
        loglike_eval = loglike(te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data)
        outputs[0][0] = np.asarray(loglike_eval)

    def grad(
        self, inputs: list[pt.TensorVariable], g: list[pt.TensorVariable]
    ) -> list[pt.TensorVariable]:
        # NEW!
        # the method that calculates the gradients - it actually returns the vector-Jacobian product
        te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data = inputs

        # Our gradient expression assumes these are scalar parameters
        if te.type.ndim != 0 or ti.type.ndim != 0 or ne.type.ndim != 0\
                or e_cur.type.ndim != 0 or flow.type.ndim != 0 or v_grad.type.ndim != 0:
            raise NotImplementedError("Gradient only implemented for scalar parameters")

        grad_wrt_te, grad_wrt_ti, grad_wrt_ne, grad_wrt_e_cur, grad_wrt_flow, grad_wrt_v_grad = self.additional_loglikegrad(te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data)

        # out_grad is a tensor of gradients of the Op outputs wrt to the function cost
        [out_grad] = g
        return [
            pt.sum(out_grad * grad_wrt_te),
            pt.sum(out_grad * grad_wrt_ti),
            pt.sum(out_grad * grad_wrt_ne),
            pt.sum(out_grad * grad_wrt_e_cur),
            pt.sum(out_grad * grad_wrt_flow),
            pt.sum(out_grad * grad_wrt_v_grad),
            # We did not implement gradients wrt to the last 3 inputs
            # This won't be a problem for sampling, as those are constants in our model
            pytensor.gradient.grad_not_implemented(self, 6, sigma),
            pytensor.gradient.grad_not_implemented(self, 7, core),
            pytensor.gradient.grad_not_implemented(self, 8, x),
            pytensor.gradient.grad_not_implemented(self, 9, data_err),
            pytensor.gradient.grad_not_implemented(self, 10, data),
        ]

class LogLikeGrad(Op):
    def make_node(self, te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data) -> Apply:
        te = pt.as_tensor(te)
        ti = pt.as_tensor(ti)
        ne = pt.as_tensor(ne)
        e_cur = pt.as_tensor(e_cur)
        flow = pt.as_tensor(flow)
        v_grad = pt.as_tensor(v_grad)
        sigma = pt.as_tensor(sigma)
        core = pt.as_tensor(core)
        x = pt.as_tensor(x)
        data_err = pt.as_tensor(data_err)
        data = pt.as_tensor(data)
        # print('make node')
        inputs = [te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data]
        # There are two outputs with the same type as data,
        # for the partial derivatives wrt to m, c
        outputs = [data.type(), data.type(), data.type(), data.type(), data.type(), data.type()]

        return Apply(self, inputs, outputs)

    def perform(self, node: Apply, inputs: list[np.ndarray], outputs: list[list[None]]) -> None:
        te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data = inputs

        # calculate gradients
        grad_wrt_te, grad_wrt_ti, grad_wrt_ne, grad_wrt_e_cur, grad_wrt_flow, grad_wrt_v_grad = finite_differences_loglike(te, ti, ne, e_cur, flow, v_grad, sigma, core, x, data_err, data)

        outputs[0][0] = grad_wrt_te
        outputs[1][0] = grad_wrt_ti
        outputs[2][0] = grad_wrt_ne
        outputs[3][0] = grad_wrt_e_cur
        outputs[4][0] = grad_wrt_flow
        outputs[5][0] = grad_wrt_v_grad